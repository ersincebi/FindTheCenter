{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from time import time\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tf2.Kfocusingtf2 import FocusedLayer1D\n",
    "from scipy.integrate import trapz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTrend(scores, name=''):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(0, len(scores)):\n",
    "        x.append(int(i))\n",
    "        y.append(int(scores[i]))\n",
    "\n",
    "    plt.plot(x, y, label=name+\" score per run\")\n",
    "\n",
    "    trend_x = x[1:]\n",
    "    z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(trend_x, p(trend_x), linestyle=\"-.\",  label=name+\" trend\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"episodes\")\n",
    "    plt.ylabel(\"scores\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig('./graphs/'+nameFormat(name)+'_trend.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(scores, choices):\n",
    "    meanByChoice = lambda lst, key: lst.count(key)/len(lst)\n",
    "\n",
    "    print('Average Score: ',sum(scores)/len(scores))\n",
    "\n",
    "    for i in set(choices):\n",
    "        print(f\"choice {i}:{meanByChoice(choices,i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossComparison(name, results):\n",
    "    plt.plot(results,  label=name+\" loss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig('./graphs/'+nameFormat(name)+'_loss_grahp.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameFormat = lambda name:name+\"-{}\".format(int(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N\n",
    "                ,mode='dense'\n",
    "                ,optimizer_s='adam'\n",
    "                ,input_shape=None\n",
    "                ,output_size=None\n",
    "                ,savedModel=None):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    if optimizer_s == 'SGDwithLR':\n",
    "        pass #optimizer = SGDwithLR(lr_dict, mom_dict,decay_dict,clip_dict)\n",
    "    elif optimizer_s == 'AdamwithCli':\n",
    "        pass #optimizer = AdamwithClip()\n",
    "    elif optimizer_s=='RMSpropwithClip':\n",
    "        pass #optimizer = RMSpropwithClip(lr=0.001, rho=0.9, epsilon=None, decay=0.0,clips=clip_dict)\n",
    "    elif optimizer_s=='adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    else:\n",
    "        pass #optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "    \n",
    "    if savedModel == None:\n",
    "        if mode=='dense':\n",
    "            layer = keras.layers.Dense(N[1], activation=\"elu\")\n",
    "        elif mode=='focused':\n",
    "            layer = FocusedLayer1D(N[1]\n",
    "                                   ,name='focus-1'\n",
    "                                   ,activation='elu'\n",
    "                                   ,init_sigma=0.25)\n",
    "            # tried values:\n",
    "            # .25, .5, .3, .2, .01, .2\n",
    "\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(N[0], activation=\"elu\", input_shape=input_shape),\n",
    "            layer,\n",
    "            keras.layers.Dense(output_size)\n",
    "        ])\n",
    "    else:\n",
    "        try:\n",
    "            restored_keras_model = keras.models.load_model(savedModel, custom_objects={'FocusedLayer1D': FocusedLayer1D})\n",
    "        except:\n",
    "            restored_keras_model = keras.models.load_model(savedModel)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data and Test Run on Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self\n",
    "                 ,envName # the gym environment will run by agent\n",
    "                 ,mode # the layer selection you may choice focused or dense\n",
    "                 ,optimizer # optimizer selction will use on a model\n",
    "                 ,seed=42 # random initializer\n",
    "                 ,N=None # each layers neuron count\n",
    "                 ,savedModel=None):\n",
    "\n",
    "        # initialize N with 32 neuron each count by default\n",
    "        if N == None:\n",
    "            N = [32,32]\n",
    "            \n",
    "        if savedModel:\n",
    "            savedModel = envName + '-' + mode + '.h5'\n",
    "\n",
    "        self.env = gym.make(envName)\n",
    "        self.env.seed(seed)\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "\n",
    "        # Discretized DQNAgent action space\n",
    "        self.is_discrete = type(self.env.action_space) == gym.spaces.discrete.Discrete\n",
    "        if not self.is_discrete:\n",
    "            shape = self.env.action_space.shape\n",
    "            low = self.env.action_space.low\n",
    "            high = self.env.action_space.high\n",
    "            self.env.action_space.n = 10\n",
    "            self.actions = np.linspace(low, high, 10, dtype=\"float32\")\n",
    "            print('The environment is not discrete')\n",
    "            print('The action space: ', self.actions.reshape(1,-1)[0])\n",
    "        \n",
    "        self.output_size = self.env.action_space.n\n",
    "\n",
    "        self.envName = envName\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.scores = []\n",
    "        self.choices = []\n",
    "        self.metrics = []\n",
    "        self.best_score_index = 0\n",
    "        self.best_score = 0\n",
    "        self.score = 0\n",
    "        \n",
    "        self.epsilon = 0.95\n",
    "        \n",
    "        self.model, self.optimizer = build_model(N=N\n",
    "                                                 ,mode=mode\n",
    "                                                 ,optimizer_s=optimizer\n",
    "                                                 ,input_shape=self.input_shape\n",
    "                                                 ,output_size=self.output_size\n",
    "                                                 ,savedModel=savedModel)\n",
    "        \n",
    "    def train(self):\n",
    "        for episode in range(EPISODES):\n",
    "            score = 0\n",
    "            best_score_index = 0\n",
    "            obs = self.env.reset()\n",
    "            for step in range(ITERATIONS):\n",
    "                self.epsilon = max(1 - episode / 500, 0.01)\n",
    "\n",
    "                obs, reward, done, action = self.play_one_step(obs)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.scores.append(score)\n",
    "\n",
    "            if step > best_score_index:\n",
    "                best_weights = self.model.get_weights()\n",
    "                best_score_index = step\n",
    "\n",
    "            print(f\"\\rEpisode: {episode+1}, Steps: {step+1}, eps: {self.epsilon}\", end=\"\")\n",
    "            if episode > 50:\n",
    "                lossValue= self.training_step()\n",
    "                self.metrics.append(lossValue)\n",
    "\n",
    "        self.model.set_weights(best_weights)\n",
    "        \n",
    "        self.saveModel()\n",
    "\n",
    "    def play_one_step(self, state):\n",
    "        state = self.setAxis(state)\n",
    "        action = self.epsilon_greedy_policy(state)\n",
    "        \n",
    "        self.choices.append(action)\n",
    "        \n",
    "        next_state, reward, done, _ = self.env.step(self.get_action(action))\n",
    "\n",
    "        self.replay_memory.append((state, self.isList(action), self.isList(reward), self.setAxis(next_state), self.isList(done)))\n",
    "        return next_state, reward, done, action\n",
    "\n",
    "    def get_action(self, action):\n",
    "        if not self.is_discrete:\n",
    "            action = [self.actions[action]]\n",
    "        return action\n",
    "\n",
    "    def setAxis(self, state):\n",
    "        if not self.is_discrete:\n",
    "            state = state.reshape(1,-1)[0]\n",
    "        return state\n",
    "    \n",
    "    def isList(self, obj):\n",
    "        try:\n",
    "            obj = obj[0]\n",
    "        finally:\n",
    "            return obj\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.output_size)\n",
    "        else:\n",
    "            Q_values = self.model.predict(state[np.newaxis])\n",
    "\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def training_step(self):\n",
    "        states, actions, rewards, next_states, dones = self.sample_experiences()\n",
    "\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "\n",
    "        target_Q_values = (rewards +\n",
    "                           (1 - dones) * discount_rate * max_next_Q_values)\n",
    "        target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "\n",
    "        mask = tf.one_hot(actions, self.output_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        if self.mode=='focused':\n",
    "            self.clipcallBack('Sigma', (0.05,1.0))\n",
    "            self.clipcallBack('Mu', (0.0,1.0))\n",
    "\n",
    "        return float(loss)\n",
    "    \n",
    "    def clipcallBack(self, varname, clips):\n",
    "        all_weights = self.model.trainable_weights\n",
    "\n",
    "        for i,p in enumerate(all_weights):\n",
    "            # print(p.name)\n",
    "            if (p.name.find(varname)>=0):\n",
    "                pval = p.numpy()\n",
    "                clipped = np.clip(pval,clips[0],clips[1])\n",
    "                p.assign(clipped)\n",
    "                # print(\"Clipped\", p.name)\n",
    "                \n",
    "    def sample_experiences(self):\n",
    "        indices = np.random.randint(len(self.replay_memory), size=batch_size)\n",
    "        batch = [self.replay_memory[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.array([experience[field_index] for experience in batch])\n",
    "            for field_index in range(5)]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def saveModel(self):\n",
    "        self.model.save('./models/{}-{}-{}.h5'.format(self.envName, self.mode, int(time())))\n",
    "        path = './results/'+self.envName+'/'\n",
    "        metrics = path+'{}-{}-metrics-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(metrics, np.array(self.metrics))\n",
    "        scores = path+'{}-{}-scores-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(scores, np.array(self.scores))\n",
    "        replay = path+'{}-{}-replay-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(replay, np.array(self.replay_memory))\n",
    "\n",
    "    def testModel(self, render = True):\n",
    "        for episode in range(EPISODES):\n",
    "            currentState = self.setAxis(self.env.reset())\n",
    "\n",
    "            print(\"============================================\")\n",
    "\n",
    "            rewardSum=0\n",
    "            for iteration in range(200):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                action = np.argmax(self.model.predict(currentState)[0])\n",
    "\n",
    "                new_state, reward, done, info = self.env.step(self.get_action(action))\n",
    "\n",
    "                new_state = self.setAxis(new_state)\n",
    "\n",
    "                currentState = new_state\n",
    "\n",
    "                rewardSum += self.isList(reward)\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} timesteps reward is {}\".format(t+1,rewardSum))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initializers\n",
    "mpl.style.use('default')\n",
    "gym.logger.set_level(40)\n",
    "np.random.seed(42)\n",
    "ops.reset_default_graph()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# loss function initializer\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "batch_size = 32 # sample size will driven to predict every episode\n",
    "discount_rate = 0.95 # determines the importance of future rewards for q value\n",
    "learning_rate = 1e-3 # determines to what extent newly acquired information overrides old information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envName = 'CartPole-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"Pendulum-v0\", \"MountainCarContinuous-v0\"] {type:\"string\"}\n",
    "# how many episodes will run\n",
    "EPISODES = 20 #@param {type: \"number\"}\n",
    "# how many steps will be taken by a episode\n",
    "ITERATIONS = 200 #@param {type: \"number\"}\n",
    "\n",
    "mode = 'dense'#@param [\"dense\", \"focused\"] {type:\"string\"}\n",
    "\n",
    "agent = DQNAgent(envName=envName\n",
    "                 ,mode=mode\n",
    "                 ,optimizer='adam'\n",
    "                 ,savedModel=True)\n",
    "\n",
    "agent.testModel()\n",
    "print('Area Under Curve',trapz(agent.scores))\n",
    "\n",
    "statistic(scores=agent.scores\n",
    "          ,choices=agent.choices)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "# results\n",
    "showTrend(scores=agent.scores, name=envName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}