{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from time import time\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tf2.Kfocusingtf2 import FocusedLayer1D\n",
    "from scipy.integrate import trapz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTrend(scores, name=''):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(0, len(scores)):\n",
    "        x.append(int(i))\n",
    "        y.append(int(scores[i]))\n",
    "\n",
    "    plt.plot(x, y, label=name+\" score per run\")\n",
    "\n",
    "    trend_x = x[1:]\n",
    "    z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(trend_x, p(trend_x), linestyle=\"-.\",  label=name+\" trend\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"episodes\")\n",
    "    plt.ylabel(\"scores\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig('./graphs/'+nameFormat(name)+'_trend.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(scores, choices):\n",
    "    meanByChoice = lambda lst, key: lst.count(key)/len(lst)\n",
    "\n",
    "    print('Average Score: ',sum(scores)/len(scores))\n",
    "\n",
    "    for i in set(choices):\n",
    "        print(f\"choice {i}:{meanByChoice(choices,i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossComparison(name, results):\n",
    "    plt.plot(results,  label=name+\" loss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig('./graphs/'+nameFormat(name)+'_loss_grahp.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameFormat = lambda name:name+\"-{}\".format(int(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N\n",
    "                ,mode='dense'\n",
    "                ,optimizer_s='adam'\n",
    "                ,input_shape=None\n",
    "                ,output_size=None\n",
    "                ,savedModel=None):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    if optimizer_s == 'SGDwithLR':\n",
    "        pass #optimizer = SGDwithLR(lr_dict, mom_dict,decay_dict,clip_dict)\n",
    "    elif optimizer_s == 'AdamwithCli':\n",
    "        pass #optimizer = AdamwithClip()\n",
    "    elif optimizer_s=='RMSpropwithClip':\n",
    "        pass #optimizer = RMSpropwithClip(lr=0.001, rho=0.9, epsilon=None, decay=0.0,clips=clip_dict)\n",
    "    elif optimizer_s=='adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    else:\n",
    "        pass #optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "    try:\n",
    "        model = keras.models.load_model(savedModel, custom_objects={'FocusedLayer1D': FocusedLayer1D})\n",
    "    except:\n",
    "        model = keras.models.load_model(savedModel)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data and Test Run on Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self\n",
    "                 ,envName # the gym environment will run by agent\n",
    "                 ,mode # the layer selection you may choice focused or dense\n",
    "                 ,optimizer # optimizer selction will use on a model\n",
    "                 ,seed=42 # random initializer\n",
    "                 ,N=None): # each layers neuron count\n",
    "\n",
    "        # initialize N with 32 neuron each count by default\n",
    "        if N == None:\n",
    "            N = [32,32]\n",
    "            \n",
    "        savedModel = envName + '-' + mode + '.h5'\n",
    "\n",
    "        self.env = gym.make(envName)\n",
    "        self.env.seed(seed)\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "\n",
    "        # Discretized DQNAgent action space\n",
    "        self.is_discrete = type(self.env.action_space) == gym.spaces.discrete.Discrete\n",
    "        if not self.is_discrete:\n",
    "            shape = self.env.action_space.shape\n",
    "            low = self.env.action_space.low\n",
    "            high = self.env.action_space.high\n",
    "            self.env.action_space.n = 10\n",
    "            self.actions = np.linspace(low, high, 10, dtype=\"float32\")\n",
    "            print('The environment is not discrete')\n",
    "            print('The action space: ', self.actions.reshape(1,-1)[0])\n",
    "        \n",
    "        self.output_size = self.env.action_space.n\n",
    "\n",
    "        self.envName = envName\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.scores = []\n",
    "        self.choices = []\n",
    "        self.metrics = []\n",
    "        self.best_score_index = 0\n",
    "        self.best_score = 0\n",
    "        self.score = 0\n",
    "        \n",
    "        self.epsilon = 0.95\n",
    "        \n",
    "        self.model, self.optimizer = build_model(N=N\n",
    "                                                 ,mode=mode\n",
    "                                                 ,optimizer_s=optimizer\n",
    "                                                 ,input_shape=self.input_shape\n",
    "                                                 ,output_size=self.output_size\n",
    "                                                 ,savedModel=savedModel)\n",
    "        \n",
    "    def run(self):\n",
    "        for episode in range(EPISODES):\n",
    "            score = 0\n",
    "            best_score_index = 0\n",
    "            obs = self.env.reset()\n",
    "            for step in range(ITERATIONS):\n",
    "                self.epsilon = max(1 - episode / 500, 0.01)\n",
    "\n",
    "                obs, reward, done, action = self.play_one_step(obs)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.scores.append(score)\n",
    "\n",
    "            print(f\"\\rEpisode: {episode+1}, Steps: {step+1}, eps: {self.epsilon}\", end=\"\")\n",
    "            if episode > 50:\n",
    "                lossValue= self.training_step()\n",
    "                self.metrics.append(lossValue)\n",
    "        \n",
    "        self.saveModel()\n",
    "\n",
    "    def play_one_step(self, state):\n",
    "        state = self.setAxis(state)\n",
    "        action = self.epsilon_greedy_policy(state)\n",
    "        \n",
    "        self.choices.append(action)\n",
    "        \n",
    "        next_state, reward, done, _ = self.env.step(self.get_action(action))\n",
    "\n",
    "        self.replay_memory.append((state, self.isList(action), self.isList(reward), self.setAxis(next_state), self.isList(done)))\n",
    "        return next_state, reward, done, action\n",
    "\n",
    "    def get_action(self, action):\n",
    "        if not self.is_discrete:\n",
    "            action = [self.actions[action]]\n",
    "        return action\n",
    "\n",
    "    def setAxis(self, state):\n",
    "        if not self.is_discrete:\n",
    "            state = state.reshape(1,-1)[0]\n",
    "        return state\n",
    "    \n",
    "    def isList(self, obj):\n",
    "        try:\n",
    "            obj = obj[0]\n",
    "        finally:\n",
    "            return obj\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.output_size)\n",
    "        else:\n",
    "            Q_values = self.model.predict(state[np.newaxis])\n",
    "\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def training_step(self):\n",
    "        states, actions, rewards, next_states, dones = self.sample_experiences()\n",
    "\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "\n",
    "        target_Q_values = (rewards +\n",
    "                           (1 - dones) * discount_rate * max_next_Q_values)\n",
    "        target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "\n",
    "        mask = tf.one_hot(actions, self.output_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        if self.mode=='focused':\n",
    "            self.clipcallBack('Sigma', (0.05,1.0))\n",
    "            self.clipcallBack('Mu', (0.0,1.0))\n",
    "\n",
    "        return float(loss)\n",
    "    \n",
    "    def clipcallBack(self, varname, clips):\n",
    "        all_weights = self.model.trainable_weights\n",
    "\n",
    "        for i,p in enumerate(all_weights):\n",
    "            # print(p.name)\n",
    "            if (p.name.find(varname)>=0):\n",
    "                pval = p.numpy()\n",
    "                clipped = np.clip(pval,clips[0],clips[1])\n",
    "                p.assign(clipped)\n",
    "                # print(\"Clipped\", p.name)\n",
    "                \n",
    "    def sample_experiences(self):\n",
    "        indices = np.random.randint(len(self.replay_memory), size=batch_size)\n",
    "        batch = [self.replay_memory[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.array([experience[field_index] for experience in batch])\n",
    "            for field_index in range(5)]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def saveModel(self):\n",
    "        path = './'\n",
    "        metrics = path+'{}-{}-metrics-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(metrics, np.array(self.metrics))\n",
    "        scores = path+'{}-{}-scores-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(scores, np.array(self.scores))\n",
    "        replay = path+'{}-{}-replay-{}.npy'.format(self.envName, self.mode, int(time()))\n",
    "        np.save(replay, np.array(self.replay_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initializers\n",
    "mpl.style.use('default')\n",
    "gym.logger.set_level(40)\n",
    "np.random.seed(42)\n",
    "ops.reset_default_graph()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# loss function initializer\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "batch_size = 32 # sample size will driven to predict every episode\n",
    "discount_rate = 0.95 # determines the importance of future rewards for q value\n",
    "learning_rate = 1e-3 # determines to what extent newly acquired information overrides old information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'focus-1', 'trainable': True, 'dtype': 'float32'}\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Episode: 6, Steps: 200, eps: 0.992"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c85a31a09327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  ,optimizer='adam')\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Area Under Curve'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrapz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d5a3a55b53a4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d5a3a55b53a4>\u001b[0m in \u001b[0;36mplay_one_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d5a3a55b53a4>\u001b[0m in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mepsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;31m# provides dtype.name.__get__, documented as returning a \"bit name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envName = 'Acrobot-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"Pendulum-v0\", \"MountainCarContinuous-v0\"] {type:\"string\"}\n",
    "# how many episodes will run\n",
    "EPISODES = 100 #@param {type: \"number\"}\n",
    "# how many steps will be taken by a episode\n",
    "ITERATIONS = 200\n",
    "\n",
    "mode = 'focused'#@param [\"dense\", \"focused\"] {type:\"string\"}\n",
    "\n",
    "agent = DQNAgent(envName=envName\n",
    "                 ,mode=mode\n",
    "                 ,optimizer='adam')\n",
    "\n",
    "agent.run()\n",
    "print('Area Under Curve',trapz(agent.scores))\n",
    "\n",
    "statistic(scores=agent.scores\n",
    "          ,choices=agent.choices)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "# results\n",
    "showTrend(scores=agent.scores, name=envName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
